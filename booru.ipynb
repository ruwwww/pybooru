{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2431a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from pybooru import Danbooru\n",
    "import urllib.request\n",
    "import os\n",
    "import tqdm\n",
    "import time\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def download_danbooru_dataset(tags):\n",
    "    client = Danbooru('danbooru') \n",
    "    MAX_WORKERS = max(1, int((os.cpu_count() or 1) * 2 / 3))\n",
    "\n",
    "    save_dir = fr'C:\\Users\\fixgk\\Kuroko\\data-resized\\{tags}'\n",
    "    \n",
    "    if os.path.exists(save_dir):\n",
    "        print(f\"Directory {save_dir} already exists. Skipping download.\")\n",
    "        return\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    all_posts = []\n",
    "\n",
    "    print(f\"Starting API fetching for tags: {tags} (Sequential)...\")\n",
    "\n",
    "    page = 1\n",
    "    while True:\n",
    "        try:\n",
    "            print(f\"Fetching page {page}...\", end='\\r')\n",
    "            \n",
    "            current_posts = client.post_list(tags=f\"{tags} -animated\", page=page, limit=100)\n",
    "            \n",
    "            if not current_posts:\n",
    "                print(f\"\\nPage {page} is empty. Reached end of results.\")\n",
    "                break\n",
    "                \n",
    "            all_posts.extend(current_posts)\n",
    "            page += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError fetching page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "    print(f\"Total posts retrieved: {len(all_posts)}\")\n",
    "\n",
    "    download_items = []\n",
    "    for post in all_posts:\n",
    "        sample_url = None\n",
    "        \n",
    "        if 'media_asset' in post and 'variants' in post['media_asset']:\n",
    "            sample_url = next(\n",
    "                (v['url'] for v in post['media_asset']['variants'] if v['type'] == 'sample'),\n",
    "                None\n",
    "            )\n",
    "        \n",
    "        if not sample_url:\n",
    "            sample_url = post.get('large_file_url') or post.get('file_url')\n",
    "\n",
    "        if sample_url:\n",
    "            ext = sample_url.split('.')[-1]\n",
    "            download_items.append({\n",
    "                'url': sample_url, \n",
    "                'id': post['id'], \n",
    "                'ext': ext\n",
    "            })\n",
    "        else:\n",
    "            # print(\"Skipping post (no URL found):\", post.get('id'))\n",
    "            pass\n",
    "\n",
    "    def download_image(item, tags_dir, max_retries=5):\n",
    "        \"\"\"Downloads a single image file with retry mechanism.\"\"\"\n",
    "        url = item['url']\n",
    "        post_id = item['id']\n",
    "        ext = item['ext']\n",
    "        file_name = os.path.join(tags_dir, f\"{post_id}_sample.{ext}\")\n",
    "        \n",
    "        if os.path.exists(file_name):\n",
    "            return False\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                urllib.request.urlretrieve(url, file_name)\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    # Exponential backoff: 2s, 4s, 8s... + random jitter\n",
    "                    wait_time = (2 ** attempt) + random.uniform(0, 1)\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Failed {post_id} after {max_retries} attempts: {e}\")\n",
    "                    return False\n",
    "\n",
    "    print(f\"Prepared {len(download_items)} links for download.\")\n",
    "    success_count = 0\n",
    "\n",
    "    if download_items:\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            # Submit tugas download\n",
    "            download_futures = [executor.submit(download_image, item, save_dir) for item in download_items]\n",
    "\n",
    "            for future in tqdm.tqdm(as_completed(download_futures), total=len(download_futures), desc=\"Downloading samples\"):\n",
    "                if future.result():\n",
    "                    success_count += 1\n",
    "\n",
    "    print(f\"Downloaded {success_count} new images at {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2f654c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting API fetching for tags: shuan_0420 (Sequential)...\n",
      "Fetching page 1...\n",
      "Error fetching page 1: HTTPSConnectionPool(host='danbooru.donmai.us', port=443): Max retries exceeded with url: /posts.json?tags=shuan_0420+-animated&page=1&limit=100 (Caused by SSLError(SSLCertVerificationError(1, \"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'danbooru.donmai.us'. (_ssl.c:1000)\")))\n",
      "Total posts retrieved: 0\n",
      "Prepared 0 links for download.\n",
      "Downloaded 0 new images at C:\\Users\\fixgk\\Kuroko\\data-resized\\shuan_0420\n"
     ]
    }
   ],
   "source": [
    "tags = \"tiger_june\" #\n",
    "\n",
    "\n",
    "download_danbooru_dataset(tags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
